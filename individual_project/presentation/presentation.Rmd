---
title: Kernel approximations <br> on large-scale problems
subtitle: Nyström method and Random Fourier features
author: Ginevra Carbone
date: "*Luglio 2018*"
output:
  ioslides_presentation:
    fig_width: 7
    incremental: yes
    smaller: yes
    widescreen: yes
    css: my.css
  html_document:
    toc: yes
editor_options:
  chunk_output_type: inline
always_allow_html: yes
---

          
## Introduction | Kernel methods

<!-- link utili:

https://www.quora.com/In-machine-learning-what-is-a-feature-map 

https://users.soe.ucsc.edu/~niejiazhong/slides/ratsch.pdf
-->

Kernel methods start from the idea of projecting data points from the input space into a high-dimensional feature space through a positive-definite **feature map** $\phi:\mathcal{X}\rightarrow\mathcal{F}$, in order to make training data easier to regress or classify.

<!-- and searching for the optimal **separating hyperplane** in that feature space. -->

<div align='center'>
<img src="figures/kernel_trick.png" width=480>
</div>

The corresponding **kernel function** $k(x,x')=<\phi(x),\phi(x')>_{\mathcal{F}}$ defines an inner product in that feature space and allows to perform the so called **kernel trick**:

<div align='center'>
One can avoid the explicit calculation of the feature map, <br> by *working only with inner products between training points*.
</div>

## Introduction | Larges-scale problems

The main limitation of kernel methods is their high **computational cost**, which is at least quadratic in the number of training points, due to storage and computation of the kernel matrix $K$.

Low-rank decompositions of $K$ (like **Cholesky decomposition**) are less expensive, but they still require its computation.

<br>
<div align="center">
*What if we avoid computing the kernel matrix?*
</div>

We are now going to explore **Nyström method** and **random Fourier features**, two methods based on **low-rank approximations** of $K$, in the special case of the radial basis function kernel

<!-- make some comparisons between them and show some interesting results on performances. -->

## Notation

- $k:\mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R}$ is a kernel function ($\mathcal{X}\subseteq \mathbb{R}^d$);
- $\mathcal{H}_k$ is the corresponding Reproducing kernel Hilbert space;
- $\mathcal{D}=\{(x_1,y_1),\dots,(x_N,y_N)\}$ is a collection of training data;
- $\mathcal{H}_{\mathcal{D}}=span(k(x_1,\cdot),\dots,k(x_N,\cdot))$ is the subspace of $\mathcal{H}_k$ generated by the training points

<!-- controllare se devo aggiungere qualcosa!! -->

## Nyström method

Take $m$ random samples with repetition $\hat{\mathcal{D}} = \{\hat{x}_1,\dots, \hat{x}_m\}$ of training points and consider the corresponding kernel matrix $\hat{K} = [k(\hat{x}_i,\hat{x}_j)]_{m\times m}$.

Supposing that $\hat{K}$ has rank $r$, the main idea of the algorithm is to **approximate $K$** by the low-rank matrix 
$$
{\hat{K}}_r = K_b \hat{K}^+ {K_b}^{-1},
$$
where $K_b=[k(x_i,\hat{x_j})]_{N\times m}$ and $\hat{K}^+$ is the Moore-Penrose pseudo inverse of $\hat{K}$ [].


Now we can derive a vector representation of data 
$$
z_n(x) = \hat{D}^{-1/2}_r \hat{V}^t_r (k(x,\hat{x}_1),\dots,k(x,\hat{x}_m))^t,
$$
where $\hat{K}_r=\hat{V}_r\hat{D}_r\hat{V}_r^t$ is the eigendecomposition of $\hat{K}_r$, and 


Thanks to this method, storage and computational complexity reduce from $O(N^2)$ and $O(N^3)$ to $O(Nm)$ and $O(Nm^2)$ respectively.


## Random Fourier features

Also known as **Random kitchen sinks** method, random Fourier features can be applied to all shift-invariant kernels, i.e. kernels of the form  $k(x,y)= k(x-y)$, since they rely upon the following result.

**Theorem.** *(Bochner) A continuous shift-invariant kernel is positive definite if and only if it is the Fourier transform of a non-negative Borel measure.*

For the sake of simplicity we are going to focus on the **Radial Basis function kernel**.

$$
k(x,y) = exp\big(-\gamma\Vert x-y\Vert_2^2\big).
$$

The theorem implies that the Fourier transform of $k$ defines a probability distribution on $\mathbb{R}^d$:

$$
p(w)=\int_{\mathbb{R^d}}e^{-2\pi i w^t \delta} k(\delta) d\delta.
$$

By projecting data onto **randomly chosen lines** with directions $w_1,\dots,w_n$ drawn from $p$, we guarantee that the inner product of two transformed points approximates the shift invariant kernel [].

The idea is to pass these projections through sine and cosine functions $z_p(x)=(sin(w_1^t x),cos(w_1^t x), \dots, sin(w_n^t x), cos(w_n^t x))$ and learn a linear machine $f(x)=w^t z_p(x)$ [4].

Basically we are searching for $f$ in the randomly generated subspace $\mathcal{H}^f=span(sin(w_1^t x),cos(w_1^t x), \dots, sin(w_n^t x), cos(w_n^t x))\subseteq \mathcal{H}_{\mathcal{D}}$ 
<!-- qui mettere immagine pag. 3 articolo 3 -->

## Applications | Results on SVM

spiegare perché faccio esempi sulla SVM

compare the results to exact kernel methods

## Theoretical differences

- data dependence
- computational complexity

- classification and regression
In goal is to learn a function $f \in \mathcal{H}_k$ by solving the optimization problem 

$$
min_{f\in\mathcal{H}_\mathcal{D}} {\frac{\lambda}{2}\Vert f \Vert_{\mathcal{H}_k}^2} + \frac{1}{N} \sum_{i=1}^N l(f(x_i),y_i)
$$
on a convex loss function $l$.

The high computational cost is due to the fact that we are searching an optimal 



## Testing phase

and test the results on classification and regression with **Support vector machines**.

<!-- faccio anche regressione? -->

Both methods are already provided by `scikit-learn` library under the names of `RBFSampler` and `Nystroem` [].

<!-- spiegare bene come sono implementate le due funzioni in python, che parametri prendono in input e come li ho scelti (anche grid search) -->

The datasets have to be large enough to show the improvements in performances.

Attenzione ai casi linearmente separabili!!!

- leaf 

train: 792 , test: 198
in questo caso ovviamente i metodi sono troppo lenti per vedere buoni risultati ma può servire a spiegare cosa voglio ottenere

- adult va già bene come primo risultato anche se piccolo

24129 training, 6033 test

- forest/ covtype

bene, ho almeno un buon esempio da fare!

- mnist original non funziona come dovrebbe... devo provare con più samples

## Empirical results

- regression vs classification performances?
- esempio PCA?

## Considerations


## References

[] Explicit feature map approximation for RBF kernels, scikit-learn library, http://scikit-learn.org/stable/auto_examples/plot_kernel_approximation.html#sphx-glr-auto-examples-plot-kernel-approximation-py 

[] P. Drineas and M. W. Mahoney, "On the Nystrom Method for Approximating a Gram Matrix for Improved Kernel-Based Learning", 2005, JMLR,
http://www.jmlr.org/papers/volume6/drineas05a/drineas05a.pdf.

[] A. Rahimi and B. Recht, "Random features for large-scale kernel machines" , 2007, NIPS, https://www.robots.ox.ac.uk/~vgg/rg/papers/randomfeatures.pdf.


[] T. Yang, Y.-F. Li, M. Mahdavi, R. Jin and Z.-H. ZhouNystrom, "Nystrom Method vs Random Fourier Features: A Theoretical and Empirical Comparison", 2012, NIPS, https://papers.nips.cc/paper/4588-nystrom-method-vs-random-fourier-features-a-theoretical-and-empirical-comparison.pdf.









