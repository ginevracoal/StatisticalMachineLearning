---
title: Kernel approximations on large-scale problems
subtitles: Nyström method and Random Fourier features
author: Ginevra Carbone
date: "*Luglio 2018*"
output:
  html_document:
    toc: yes
  ioslides_presentation:
    fig_width: 7
    incremental: yes
    smaller: yes
    widescreen: yes
editor_options:
  chunk_output_type: inline
always_allow_html: yes
---

          
## Introduction 

### Kernel methods

<!-- non posso spiegare pure cosa sono i kernel mthods... in caso lo faccio alla fine, vado in medias res intanto -->

Kernel methods are based on the idea of projecting data points from the input space into a high-dimensional feature space, through a feature map $\phi:\mathcal{X}\rightarrow\mathcal{F}$.

<!-- and searching for the optimal **separating hyperplane** in that feature space. -->

<!-- <img src="figures/kernel_trick.png" width=380> -->

The corresponding kernel function $k(x,x')=<\phi(x),\phi(x')>_{\mathcal{F}}$ 
allows the representation of input data as a **Gram matrix**. This **kernel trick** is used to solve many machine learning problems.


### Larges-scale problems

The main limitation of kernel methods is their high **computational cost**, which is at least quadratic in the number of training points, due to storage and computation of the kernel matrix $K$.

Low-rank decompositions of $K$ (like **Cholesky decomposition**) are less expensive, but they still require its computation.

<!-- dire qualcosa di più su cholesky.. -->

An alternative is to simply avoid the computation of the kernel matrix. 

We are now going to explore **Nyström method** and **random Fourier features**, two methods based on **low-rank approximations** of $K$, make some comparisons between them and show some interesting results on performances.

## Notation

- $k:\mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R}$ is a kernel function ($\mathcal{X}\subseteq \mathbb{R}^d$);
- $\mathcal{H}_k$ is the corresponding Reproducing kernel Hilbert space;
- $\mathcal{D}=\{(x_1,y_1),\dots,(x_N,y_N)\}$ is a collection of training data;
- $\mathcal{H}_{\mathcal{D}}=span(k(x_1,\cdot),\dots,k(x_N,\cdot))$ is the subspace of $\mathcal{H}_k$ generated by the training points





## Nyström method

Take $m$ random samples with repetition $\hat{\mathcal{D}} = \{\hat{x}_1,\dots, \hat{x}_m\}$ of training points and consider the corresponding kernel matrix $\hat{K} = [k(\hat{x}_i,\hat{x}_j)]_{m\times m}$.

Supposing that $\hat{K}$ has rank $r$, the main idea of the algorithm is to **approximate $K$** by the low-rank matrix 
$$
{\hat{K}}_r = K_b \hat{K}^+ {K_b}^{-1},
$$
where $K_b=[k(x_i,\hat{x_j})]_{N\times m}$ and $\hat{K}^+$ is the Moore-Penrose pseudo inverse of $\hat{K}$ [].

<!-- storage and computational complexity reduce to $O(Nm)$ and $O(Nm^2)$ respectively. -->

Now we can derive a vector representation of data 
$$
z_n(x) = \hat{D}^{-1/2}_r \hat{V}^t_r (k(x,\hat{x}_1),\dots,k(x,\hat{x}_m))^t,
$$
where $\hat{K}_r=\hat{V}_r\hat{D}_r\hat{V}_r^t$ is the eigendecomposition of $\hat{K}_r$, and 

## Random Fourier features

Also known as **Random kitchen sinks**, random Fourier features can be applied to all shift-invariant kernels, i.e. kernels of the form  $k(x,y)= k(x-y)$, since they rely upon the following result.

**Theorem.** *(Bochner) A continuous shift-invariant kernel is positive definite iff it is the Fourier transform of a non-negative Borel measure.*

For the sake of simplicity we are going to focus on the **Radial Basis function kernel**.

$$
k(x,y) = exp\big(-\gamma\Vert x-y\Vert_2^2\big).
$$

The theorem implies that the Fourier transform of $k$ defines a probability distribution on $\mathbb{R}^d$:

$$
p(w)=\int_{\mathbb{R^d}}e^{-2\pi i w^t \delta} k(\delta) d\delta.
$$

By projecting data onto **randomly chosen lines** with directions $w_1,\dots,w_n$ drawn from $p$, we guarantee that the inner product of two transformed points approximates the shift invariant kernel [].

The idea is to pass these projections through sine and cosine functions $z_p(x)=(sin(w_1^t x),cos(w_1^t x), \dots, sin(w_n^t x), cos(w_n^t x))$ and learn a linear machine $f(x)=w^t z_p(x)$ [4].

Basically we are searching for $f$ in the randomly generated subspace $\mathcal{H}^f=span(sin(w_1^t x),cos(w_1^t x), \dots, sin(w_n^t x), cos(w_n^t x))\subseteq \mathcal{H}_{\mathcal{D}}$ 
<!-- qui mettere immagine pag. 3 articolo 3 -->

## Theoretical differences

- data dependence
- computational complexity

- classification and regression
In goal is to learn a function $f \in \mathcal{H}_k$ by solving the optimization problem 

$$
min_{f\in\mathcal{H}_\mathcal{D}} {\frac{\lambda}{2}\Vert f \Vert_{\mathcal{H}_k}^2} + \frac{1}{N} \sum_{i=1}^N l(f(x_i),y_i)
$$
on a convex loss function $l$.

The high computational cost is due to the fact that we are searching an optimal 



## Testing phase


and test the results on classification and regression with **Support vector machines**.

<!-- faccio anche regressione? -->

Both methods are already provided by `scikit-learn` library under the names of `RBFSampler` and `Nystroem` [].

<!-- spiegare bene come sono implementate le due funzioni in python, che parametri prendono in input e come li ho scelti (anche grid search) -->

The datasets have to be large enough to show the improvements in performances.

- leaf 

train: 792 , test: 198
in questo caso ovviamente i metodi sono troppo lenti per vedere buoni risultati ma può servire a spiegare cosa voglio ottenere

- adult va già bene come primo risultato anche se piccolo

24129 training, 6033 test

- forest/ covtype

bene, ho almeno un buon esempio da fare!

- mnist original non funziona come dovrebbe... devo provare con più samples

## Empirical results

- regression vs classification performances?
- esempio PCA?

## Considerations


## References

[] Explicit feature map approximation for RBF kernels, scikit-learn library, http://scikit-learn.org/stable/auto_examples/plot_kernel_approximation.html#sphx-glr-auto-examples-plot-kernel-approximation-py 

[] P. Drineas and M. W. Mahoney, "On the Nystrom Method for Approximating a Gram Matrix for Improved Kernel-Based Learning", 2005, JMLR,
http://www.jmlr.org/papers/volume6/drineas05a/drineas05a.pdf.

[] A. Rahimi and B. Recht, "Random features for large-scale kernel machines" , 2007, NIPS, https://www.robots.ox.ac.uk/~vgg/rg/papers/randomfeatures.pdf.


[] T. Yang, Y.-F. Li, M. Mahdavi, R. Jin and Z.-H. ZhouNystrom, "Nystrom Method vs Random Fourier Features: A Theoretical and Empirical Comparison", 2012, NIPS, https://papers.nips.cc/paper/4588-nystrom-method-vs-random-fourier-features-a-theoretical-and-empirical-comparison.pdf.









