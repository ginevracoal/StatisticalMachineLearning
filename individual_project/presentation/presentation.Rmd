---
title: Kernel approximations <br> on large-scale problems
subtitle: Nyström method and Random Fourier features
author: Ginevra Carbone
date: "*Luglio 2018*"
output:
  ioslides_presentation:
    fig_width: 7
    incremental: yes
    smaller: yes
    widescreen: yes
    css: my.css
  html_document:
    toc: yes
editor_options:
  chunk_output_type: inline
always_allow_html: yes
---

## Introduction | Kernel methods {.build}

<!-- link utili:

https://www.quora.com/In-machine-learning-what-is-a-feature-map 

https://users.soe.ucsc.edu/~niejiazhong/slides/ratsch.pdf
-->

Kernel methods start from the idea of projecting data points from the input space into a high-dimensional feature space through a positive-definite **feature map** $\phi:\mathcal{X}\rightarrow\mathcal{F}$, in order to make training data easier to regress or classify.

<!-- and searching for the optimal **separating hyperplane** in that feature space. -->

<div class='columns-2'>

<img src="figures/kernel_trick.png" width=480>

<br>

The corresponding **kernel function** $k(x,x')=<\phi(x),\phi(x')>_{\mathcal{F}}$ defines an inner product in that feature space and allows to perform the so called **kernel trick**:

one can avoid the explicit calculation of the feature map, by *working only with inner products between training points*.

</div>

## Introduction | Larges-scale problems {.build}

The main limitation of kernel methods is their high **computational cost**, which is at least quadratic in the number of training points, due to storage and computation of the kernel matrix $K$.

Low-rank decompositions of $K$ (like **Cholesky decomposition**) are less expensive , but they still require its computation.

<br>
<div align="center">
*What if we avoid computing the kernel matrix?*


We are now going to explore **Nyström method** and **random Fourier features**, <br> two methods based on **low-rank approximations** of $K$.
</div>

For the sake of simplicity, we'll focus on the **Radial Basis function kernel**
$$
k(x,y) = exp\big(-\gamma\Vert x-y\Vert_2^2\big),
$$
in which case both methods are valid.

<!-- make some comparisons between them and show some interesting results on performances. -->

# The boring stuff

## Notation {.build}

- $k:\mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R}$ is a kernel function $\big(\mathcal{X}\subseteq \mathbb{R}^d\big)$;
- $\mathcal{H}_k$ is the corresponding Reproducing kernel Hilbert space;
- $\mathcal{D}=\{(x_1,y_1),\dots,(x_N,y_N)\}$ is a collection of training data;
- $\mathcal{H}_{\mathcal{D}}=span(k(x_1,\cdot),\dots,k(x_N,\cdot))$ is the subspace of $\mathcal{H}_k$ generated by the training points.



<br>

Our general goal is to efficiently solve a regularized least squares problem 
$$
\min_{f\in\mathcal{\mathcal{H}_\mathcal{D}}} \frac{\lambda}{2} \Vert f\Vert^2_{\mathcal{H}_\mathcal{D}} + \frac{1}{N} \sum_{i=1}^N l(f(x_i),y_i)
$$
by restricting the solution to an approximate **randomly generated** functional subspace
$\mathcal{H}_a\subseteq \mathcal{H}_\mathcal{D}$.

<!-- uso H_d invece di H_k per il representer theorem!! -->




## Nyström method {.build}

Take $m$ **random samples** (uniform sampling without replacement is the most used approach) $\hat{\mathcal{D}} = \{\hat{x}_1,\dots, \hat{x}_m\}$ of training points and consider the corresponding kernel matrix $\hat{K} = [k(\hat{x}_i,\hat{x}_j)]_{m\times m}$.

The main idea of the algorithm is to **approximate $K$** by the low-rank  matrix
$$
{\tilde{K}}_r = K_b \hat{K}^+ {K_b}^{-1},
$$
where $rank(\hat{K}) = r<m$, $K_b=[k(x_i,\hat{x}_j)]_{N\times m}$ and $\hat{K}^+$ is the Moore-Penrose pseudo inverse of $\hat{K}$ *[1, p.2]*.


<!--principale: http://www.jmlr.org/papers/volume6/drineas05a/drineas05a.pdf -->
<!-- https://www.cs.cmu.edu/~muli/file/nystrom_icml10.pdf -->
<!-- http://www.jmlr.org/papers/volume13/kumar12a/kumar12a.pdf -->

Given the eigendecomposition $\tilde{K}_r=\hat{V}_r\hat{D}_r\hat{V}_r^t$
we can now derive a vector representation of data 
$$
z_n(x) = \hat{D}^{-1/2}_r \hat{V}^t_r (k(x,\hat{x}_1),\dots,k(x,\hat{x}_m))^t \in \mathbb{R}^r.
$$


## Nyström method {.build}

Thanks to this vector representation we can learn a linear machine $f(x)=w^t z_n(x)$ by solving 
<!-- spiegare cosa sono le matrici di eigendecomposition -->

$$
\min_{w\in\mathbb{R}^r} \frac{\lambda}{2} \Vert w\Vert^2_2 + \frac{1}{N} \sum_{i=1}^N l(w^t z_n(x_i),y_i),
$$
which is equivalent to our original problem, under the assumption of searching the linear machine into an approximate subspace of $\mathcal{H}_\mathcal{D}$ *[2, p.3]*.

<div align='center'>
This method reduces storage and computational **complexity** of the Gram matrix <br> from $O(N^2)$ and $O(N^3)$ to $O(Nr)$ and $O(Nr^2)$ respectively.
<!-- so they are linear in N! -->
</div>


## Random Fourier features {.build}

Also known as **Random kitchen sinks** method, random Fourier features can be applied to all shift-invariant kernels, i.e. kernels of the form  $k(x,y)= k(x-y)$, since it relies upon the following result.

**Theorem.** *(Bochner) A continuous shift-invariant kernel is positive definite if and only if it is the Fourier transform of a non-negative Borel measure.*

In the case of gaussian kernel, the theorem implies that the Fourier transform of $k$ defines a multivariate gaussian probability distribution on $\mathbb{R}^d$:

<!-- controllare questa frase!!! -->

$$
p(w)=\int_{\mathbb{R^d}}e^{-2\pi i w^t \delta} k(\delta) d\delta\sim \mathcal{N}(0,I/\sigma^2)
$$

By projecting data onto **randomly chosen lines** with directions $w_1,\dots,w_n$ drawn from $p$, we guarantee that the inner product of two transformed points approximates the shift invariant kernel $k$ *[3]*.

## Random Fourier features {.build}

The idea is to pass these projections through sine and cosine functions $$z_p(x)=(sin(w_1^t x),cos(w_1^t x), \dots, sin(w_n^t x), cos(w_n^t x))$$ 

<div align='center'>
<img src="figures/random_fourier.png" height=200>
</div>

and then learn a linear machine $f(x)=w^t z_p(x)$ from

$$
\min_{w\in\mathbb{R}^{2m}} \frac{\lambda}{2} \Vert w\Vert^2_2 + \frac{1}{N} \sum_{i=1}^N l(w^t z_p(x_i),y_i).
$$

<!-- complessità computazionale??? -->


## Theoretical comparison {.build}

<div align='center'>
*The high computational cost of kernel methods is due to  searching  <br>  for an optimal solution over a **large space** * $\mathcal{H}_\mathcal{D}$.

<br>

Both methods solve this problem by considering a randomly generated subspace $\mathcal{H}_a\subseteq \mathcal{H}_\mathcal{D}$. 

The main difference is in the **sampling method** chosen for the basis functions spanning this space: 
random Fourier features samples from a **data-independent** distribution, <br> while Nystroem method from a **data-dependent** distribution.
</div> 

<br>

This difference implies a significant impact on the **performances**, because in the case of a large *eigengap* Nystroem method is quicker at discovering the subspace spanned by the top eigenvectors *[2, p.4]*.

<!-- meaning that the first eigenvalues of the kernel matrix are much larger that the others -->

# Testing phase

## Empirical results | SVM classification {.build}

<div align='center'>
*Let's now make this discussion concrete* <br> and show some results on **SVM classifiers**.
</div>

<br>

Both methods are provided by `scikit-learn` library as `RBFSampler` and `Nystroem` *[4]*. 
<!-- By default nystroem uses the rbf  -->

I tested them against the **exact kernel methods** on **large enough** datasets, in order to show the improvements in performances.


``` python
# SVM classifiers
kernel_svm = svm.SVC(kernel='rbf', gamma=gamma, C=C)
linear_svm = svm.SVC(kernel='linear', C=C)

# the two methods
random_fourier = RBFSampler(gamma=gamma, random_state=1)
nystroem = Nystroem(gamma=gamma, random_state=1)

# pipelines for kernel approximations
fourier_svm = pipeline.Pipeline([("feature_map", random_fourier),("svm", linear_svm)])
nystroem_svm = pipeline.Pipeline([("feature_map", nystroem),("svm", linear_svm)])
```


<!-- spiegare che i dati vanno centrati (pca) e grid search per gamma -->

<!-- dire anche le caratteristiche di galileo -->

## Empirical results | SVM on Adult dataset {.build}


<div align='center'>
Binary classification on 123 features

30162 training - 15060 test
</div>

<!-- linear accuracy = rbf accuracy means that data is linearly sep -->

<div class='columns-2'>
<img src="../code/plots/adult_30000_accuracy.png" width=490>

<br>
<br>

<img src="../code/plots/adult_30000_timescale.png" width=490>
</div>

## Empirical results | SVM on Covtype dataset

<div align='center'>
40000 training, 10000 test
</div>

<!-- classificazione binaria -->
<!-- linear accuracy = rbf accuracy (linearly sep) -->

<div class='columns-2'>
<img src="../code/plots/covtype_50000_accuracy.png" width=490>

<br>
<br>

<img src="../code/plots/covtype_50000_timescale.png" width=490>
</div>

## Empirical results | SVM on MNIST dataset

## Empirical results | PCA ?



## Final considerations {.build}

## References


[1] P.Drineas, M. W. Mahoney, "On the Nystrom Method for Approximating a Gram Matrix for Improved Kernel-Based Learning", 2005, http://www.jmlr.org/papers/volume6/drineas05a/drineas05a.pdf

[2] T. Yang, Y.-F. Li, M. Mahdavi, R. Jin and Z.-H. ZhouNystrom, "Nystrom Method vs Random Fourier Features: A Theoretical and Empirical Comparison", 2012, NIPS, https://papers.nips.cc/paper/4588-nystrom-method-vs-random-fourier-features-a-theoretical-and-empirical-comparison.pdf.

[3] A. Rahimi and B. Recht, "Random features for large-scale kernel machines" , 2007, NIPS, https://www.robots.ox.ac.uk/~vgg/rg/papers/randomfeatures.pdf.

[4] Explicit feature map approximation for RBF kernels, scikit-learn library, http://scikit-learn.org/stable/auto_examples/plot_kernel_approximation.html#sphx-glr-auto-examples-plot-kernel-approximation-py 



# Thank you for your attention
          










