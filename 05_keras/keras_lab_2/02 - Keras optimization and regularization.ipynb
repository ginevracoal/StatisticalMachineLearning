{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimization and Regularization in Keras\n",
    "\n",
    "### Goals: \n",
    "- Optimization: explore optimization and regularization in `Keras`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# display figures in the notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "N = X_train.shape[1]\n",
    "H = 100\n",
    "K = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(H, input_dim=N))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(H))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(K))\n",
    "model.add(Activation(\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "The basic method for optimization is SGD. The basic implementation in Keras exposes some add-ons, like Momentum and Nesterov Momentum.\n",
    "\n",
    "Expore possibilities with:\n",
    "`optimizers.SGD?`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers.SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.SGD(lr=0.1),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1527/1527 [==============================] - 0s 159us/step - loss: 1.0293 - acc: 0.7492\n",
      "Epoch 2/15\n",
      "1527/1527 [==============================] - 0s 46us/step - loss: 0.2376 - acc: 0.9522\n",
      "Epoch 3/15\n",
      "1527/1527 [==============================] - 0s 50us/step - loss: 0.1384 - acc: 0.9686\n",
      "Epoch 4/15\n",
      "1527/1527 [==============================] - 0s 47us/step - loss: 0.0916 - acc: 0.9817\n",
      "Epoch 5/15\n",
      "1527/1527 [==============================] - 0s 54us/step - loss: 0.0656 - acc: 0.9915\n",
      "Epoch 6/15\n",
      "1527/1527 [==============================] - 0s 52us/step - loss: 0.0511 - acc: 0.9935\n",
      "Epoch 7/15\n",
      "1527/1527 [==============================] - 0s 52us/step - loss: 0.0393 - acc: 0.9941\n",
      "Epoch 8/15\n",
      "1527/1527 [==============================] - 0s 54us/step - loss: 0.0311 - acc: 0.9980\n",
      "Epoch 9/15\n",
      "1527/1527 [==============================] - 0s 53us/step - loss: 0.0256 - acc: 0.9987\n",
      "Epoch 10/15\n",
      "1527/1527 [==============================] - 0s 51us/step - loss: 0.0210 - acc: 0.9987\n",
      "Epoch 11/15\n",
      "1527/1527 [==============================] - 0s 56us/step - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 12/15\n",
      "1527/1527 [==============================] - 0s 60us/step - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 13/15\n",
      "1527/1527 [==============================] - 0s 54us/step - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 14/15\n",
      "1527/1527 [==============================] - 0s 51us/step - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "1527/1527 [==============================] - 0s 53us/step - loss: 0.0106 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,  epochs=15, batch_size=32);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has other types of optimization algorithms. Explore possibilities in the online documentation:\n",
    "\n",
    "- Replace the SGD optimizer by the Adam optimizer from keras and run it\n",
    "  with the default parameters.\n",
    "\n",
    "- Add another hidden layer and use the \"Rectified Linear Unit\" for each\n",
    "  hidden layer. Can you still train the model with Adam with its default global\n",
    "  learning rate?\n",
    "\n",
    "- Bonus: try the Adadelta optimizer (no learning rate to set).\n",
    "\n",
    "Hint: use `optimizers.<TAB>` to tab-complete the list of implemented optimizers in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: forward pass and generalization\n",
    "\n",
    "- Compute predictions on test set using `model.predict_classes(...)`\n",
    "- Evaluate the model using `model.evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise: impact of initialization\n",
    "\n",
    "Let us now study the impact of a bad initialization when training\n",
    "a deep feed forward network.\n",
    "\n",
    "By default Keras dense layers use the \"Glorot Uniform\" initialization\n",
    "strategy to initialize the weight matrices:\n",
    "\n",
    "- each weight coefficient is randomly sampled from [-scale, scale]\n",
    "- scale is proportional to $\\frac{1}{\\sqrt{n_{in} + n_{out}}}$\n",
    "\n",
    "This strategy is known to work well to initialize deep neural networks\n",
    "with \"tanh\" or \"relu\" activation functions and then trained with\n",
    "standard SGD.\n",
    "\n",
    "To assess the impact of initialization let us plug an alternative init\n",
    "scheme into a 2 hidden layers networks with \"tanh\" activations.\n",
    "For the sake of the example let's use normal distributed weights\n",
    "with a manually adjustable scale (standard deviation) and see the\n",
    "impact the scale value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1527/1527 [==============================] - 0s 150us/step - loss: 2.3035\n",
      "Epoch 2/10\n",
      "1527/1527 [==============================] - 0s 51us/step - loss: 2.3027\n",
      "Epoch 3/10\n",
      "1527/1527 [==============================] - 0s 45us/step - loss: 2.3022\n",
      "Epoch 4/10\n",
      "1527/1527 [==============================] - 0s 47us/step - loss: 2.3002\n",
      "Epoch 5/10\n",
      "1527/1527 [==============================] - 0s 49us/step - loss: 2.2914\n",
      "Epoch 6/10\n",
      "1527/1527 [==============================] - 0s 44us/step - loss: 2.1643\n",
      "Epoch 7/10\n",
      "1527/1527 [==============================] - 0s 45us/step - loss: 1.8515\n",
      "Epoch 8/10\n",
      "1527/1527 [==============================] - 0s 44us/step - loss: 1.5621\n",
      "Epoch 9/10\n",
      "1527/1527 [==============================] - 0s 45us/step - loss: 1.1486\n",
      "Epoch 10/10\n",
      "1527/1527 [==============================] - 0s 51us/step - loss: 0.8959\n"
     ]
    }
   ],
   "source": [
    "from keras import initializers\n",
    "\n",
    "normal_init = initializers.RandomNormal(stddev=0.01)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(H, input_dim=N, kernel_initializer=normal_init))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(K, kernel_initializer=normal_init))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(K, kernel_initializer=normal_init))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=0.1),\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "- Try the following initialization schemes and see whether\n",
    "  the SGD algorithm can successfully train the network or\n",
    "  not:\n",
    "  \n",
    "  - a very small e.g. `scale=1e-3`\n",
    "  - a larger scale e.g. `scale=1` or `10`\n",
    "  - initialize all weights to 0 (constant initialization)\n",
    "  \n",
    "- What do you observe? Can you find an explanation for those\n",
    "  outcomes?\n",
    "\n",
    "- Are better solvers such as SGD with momentum or Adam able\n",
    "  to deal better with such bad initializations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "Keras implements several forms of regularization. \n",
    "Most forms of regularization are implemented as layers. This is the case for Dropout, for Noise Injection, for Batch Normalization. \n",
    "\n",
    "One of the most used techniques in Deep Learning is Dropout. Dropout is implemented in Keras as an extra layer, which can be added after a normal layer, and works on its output (or on the input of the next layer).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dropout\n",
    "\n",
    "Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "keras.layers.core.Dropout(rate, noise_shape=None, seed=None)\n",
    "```\n",
    "\n",
    "Applies Dropout to the input.\n",
    "\n",
    "Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "\n",
    "Arguments\n",
    "\n",
    "* rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "* noise_shape: 1D integer tensor representing the shape of the binary dropout mask that will be multiplied with the input. For instance, if your inputs have shape  (batch_size, timesteps, features) and you want the dropout mask to be the same for all timesteps, you can use noise_shape=(batch_size, 1, features).\n",
    "* seed: A Python integer to use as random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Keras guarantess automatically that this layer is **not** used in **Inference** (i.e. Prediction) phase\n",
    "(thus only used in **training** as it should be!)\n",
    "\n",
    "See `keras.backend.in_train_phase` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: dropout\n",
    "Add dropout layers to the previous model (defining a new model), use a dropout rate of 0.2 - or explore some alternatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other regularization and normalization in Keras\n",
    "Among the most used regularization layers, we have:\n",
    "- `keras.layers.GaussianNoise(stddev)`, which applies additive zero-centered Gaussian noise to its input.\n",
    "- `keras.layers.BatchNormalization`, which implements Batch Normalization. Check its options in the [Keras web page](https://keras.io/layers/normalization/)\n",
    "\n",
    "\n",
    "There are also other regularizations that can be useful. Layers having weights, like the `Dense` layer, has options to introduce L1 or L2 penalties on weights (`kernel_regularizer`) or activations (`activity_regularizer`). Possible values here are the following objectes `keras.regularizers.l1(alpha)`, `keras.regularizers.l2(alpha)`, and  `keras.regularizers.l1_l2(alpha)`. You can implement your own.\n",
    "\n",
    "\n",
    "The control of gradient norm (i.e. gradient clipping) can be set directly on the optimizer, using options `clipnorm` and `clipval`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise  \n",
    "Experiment with these different forms or regularization, one at time, to better understand their effect (use the MNIST dataset if your computer allows). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Early stopping is the most used regularizer. But how to use it? \n",
    "The solution are Keras Callbacks.\n",
    "\n",
    "\n",
    "A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks) to the .fit() method of the Sequential or Model classes. The relevant methods of the callbacks will then be called at each stage of the training.\n",
    "\n",
    "\n",
    "There are some default callbacks available in Keras, which you can use. Check the [Keras documentation page](https://keras.io/callbacks/) for the full list:\n",
    "- `ModelCheckpoint`: save the model after every epoch;\n",
    "- `EarlyStopping`: stop training when a monitored quantity has stopped improving;\n",
    "- `LearningRateScheduler`: allows to change the lerning rate after each epoch.  \n",
    "\n",
    "`EarlyStopping` takes the following parameters:\n",
    "- `monitor`: quantity to be monitored.\n",
    "- `min_delta`: minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "- `patience`: number of epochs with no improvement after which training will be stopped.\n",
    "- `verbose`: verbosity mode.\n",
    "- `mode`: one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing validation data for Early Stopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-c7185d74222f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#task: improve the optimizer!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m model.compile(loss='categorical_crossentropy', optimizer=SGD(), \n\u001b[0m\u001b[1;32m     15\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SGD' is not defined"
     ]
    }
   ],
   "source": [
    "#Early Stopping Example. \n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#task: improve the optimizer!\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#increase if your hardware allows that!\n",
    "epochs = 10\n",
    "    \n",
    "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), nb_epoch=epochs, \n",
    "          batch_size=128, verbose=True, callbacks=[early_stop]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
